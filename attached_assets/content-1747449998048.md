Full-Stack Architecture and Integration Analysis for Persistent Data and Modularity

Content may be unverified or unsafe. [Report](https://openai.com/form/report-content/)

ChatGPTTry ChatGPT

[ChatGPT](https://chatgpt.com/?utm_src=deep-research-sharing)

# Architecture Analysis and Recommendations for Arizona\_Practice App

The Arizona\_Practice app is a React/Vite frontend with an Express/Node backend (using Drizzle/Neon for a Postgres DB). Currently, data (CSV uploads and computed analytics) live only in the client’s state and localStorage; although the backend has endpoints for uploads, the front end does not fetch persisted data from it. We recommend reorganizing the stack to enforce clear separation of concerns, durable storage, and scalable architecture.

## 1\. Modularity

- **Clear component and module boundaries.** The code splits functionality into separate files – e.g. CSV parsing ( `csv-parser.ts`, `simplified-monthly-parser.ts`), state management ( `data-store.ts`), and UI components (e.g. `NetMarginChart.tsx` [github.com](https://github.com/0xGonz/Arizona_Practice/blob/468135e3ab3e7021484294797b8d41c4179e9e8b/client/src/components/dashboard/net-margin-chart.tsx#L12-L21)). Charts and cards are implemented as standalone React components, which is good. However, the **Zustand store** currently mixes many responsibilities (raw data, derived analytics, and UI state) in one file[github.com](https://github.com/0xGonz/Arizona_Practice/blob/8904ebb566592c0a7375f7d9b30b1ae11b0ef148/client/src/store/data-store.ts#L315-L322). For example, the store holds `annualData`, `monthlyData`, as well as derived arrays ( `revenueMix`, `marginTrend`, `topPerformers`, etc.) all together[github.com](https://github.com/0xGonz/Arizona_Practice/blob/8904ebb566592c0a7375f7d9b30b1ae11b0ef148/client/src/store/data-store.ts#L315-L322). This coupling can be reduced by splitting state logic into separate modules or hooks (e.g. one slice for raw CSV data, another for analytics) or using a more structured state container.

- **Decouple parsing from UI logic.** The upload component ( `CSVUpload.tsx`) currently handles both sending the file to the server **and** parsing it on the client (using Papa.parse)[github.com](https://github.com/0xGonz/Arizona_Practice/blob/8904ebb566592c0a7375f7d9b30b1ae11b0ef148/client/src/components/upload/csv-upload.tsx#L48-L55)[github.com](https://github.com/0xGonz/Arizona_Practice/blob/8904ebb566592c0a7375f7d9b30b1ae11b0ef148/client/src/components/upload/csv-upload.tsx#L72-L81). This conflates upload logic with CSV processing. Instead, consider having the server parse CSV content so that parsing logic lives in one place. For example, after upload the backend could run `parseMonthlyCSV` (or a similar parser) and return the structured result to the client. This would isolate parsing in a shared module (potentially a common “parser” library used by server and client) and keep the upload component focused on file transfer. It would also simplify the client: instead of maintaining both raw and parsed data in state, the frontend could fetch already-parsed JSON from an API endpoint.

- **Isolate business logic in utilities.** Complex analytics (e.g. doctor performance, department summaries) are handled by functions in `performance-utils.ts` or `department-utils-new.ts`. These are reasonably modular. We should continue this pattern and ensure UI components simply call these pure functions. For example, the doctor-performance page uses `extractDoctorPerformanceData` and then renders it. If these functions grow, consider grouping them into a service layer or moving some to backend APIs (see below) so that logic isn’t duplicated between UI and server.

- **Enhance the CSV parser.** The `parseMonthlyCSV` function (in `simplified-monthly-parser.ts`) currently returns a flat list of items with `depth` and `entityValues`, but **does not include a nested hierarchy**. Indeed, the store’s `processCSVData` merges the parser output directly into state with keys like `lineItems`, `entityColumns`, etc., without building a tree[github.com](https://github.com/0xGonz/Arizona_Practice/blob/8904ebb566592c0a7375f7d9b30b1ae11b0ef148/client/src/store/data-store.ts#L579-L587). As noted in the repo’s own review notes, this means the UI can only show flat tables (the hierarchical view never appears)[github.com](https://github.com/0xGonz/Arizona_Practice/blob/8904ebb566592c0a7375f7d9b30b1ae11b0ef148/attached_assets/content-1747443284525.md#L20-L28). To fix this, the parser or a separate utility should construct a nested tree (e.g. by calling a `buildHierarchy()` function) and return it along with the flat data. For example, change the parser’s return to `{ flat: [...], nested: [...], meta: {...} }` and store all parts. This isolates the hierarchy-building logic and keeps raw vs. nested data separate.

- **Layered store structure.** In `data-store.ts`, the `monthlyData` object is a map of months to their data. Each month entry currently contains keys `'e'` and `'o'` for the two file types[github.com](https://github.com/0xGonz/Arizona_Practice/blob/8904ebb566592c0a7375f7d9b30b1ae11b0ef148/client/src/store/data-store.ts#L579-L587). Inside each, the code spreads `processedData` (which includes `lineItems`, etc.) and adds a `raw` copy. This is reasonable, but could be refactored so that state holds a clear structure like `{ flat, nested, meta, raw }` rather than merging raw with parsed fields. That way rendering code can refer to, say, `monthlyData[month].e.flat` vs `.nested`. This avoids implicit coupling of raw vs. parsed in the same object.


In summary, the code is fairly well-separated into modules, but we can improve modularity by: separating store slices, isolating file-upload vs. parsing logic, returning richer structures from parsers, and possibly co-locating shared logic (e.g. CSV schema definitions) so it’s not duplicated. All UI rendering components (charts, cards, tables) should remain independent, simply consuming data from the store or APIs.

## 2\. Database Connectivity

Currently, the backend defines a storage interface with both an in-memory ( `MemStorage`) and a database-backed ( `DatabaseStorage`) implementation (using Drizzle ORM and a Postgres table `csv_uploads`). However, unless a `DATABASE_URL` is provided, it falls back to the in-memory store, so data is lost on restart. Even when using the DB, the code only **stores the raw CSV content** in the `content` column (and metadata in `type`, `month`, etc.)[github.com](https://github.com/0xGonz/Arizona_Practice/blob/8904ebb566592c0a7375f7d9b30b1ae11b0ef148/client/src/lib/department-utils-new.ts#L6-L14)[github.com](https://github.com/0xGonz/Arizona_Practice/blob/8904ebb566592c0a7375f7d9b30b1ae11b0ef148/client/src/lib/department-utils-new.ts#L72-L80). No parsed line items or nested structures are saved.

As a result, **uploaded data is not truly persistent or queryable**. The client never reads from the database; it only reads from its local state and localStorage. To fix this, we should:

- **Use the database for parsed data.** Extend the schema to save parsed results. For example, create a `line_items` table with columns like `upload_id (FK)`, `name`, `depth`, `entityValues (JSONB)`, `summaryValue`, etc. Store each row of the CSV (flat structure) in this table. Also store the nested structure either as a separate table (self-referencing parent-child rows) or as a JSONB column on the upload. This way, the entire P&L hierarchy is persisted per upload.

- **Associate data with user/month/type.** The current `csv_uploads` table has `type` (‘annual’, ‘monthly-e’, ‘monthly-o’) and an optional `month` string. To support multi-user or multi-month retrieval, add columns such as `user_id` (nullable or foreign key), `year`, etc. For example, a record could have `(user: 'alice', year: 2025, month: 'january', type: 'monthly-e')`. Then the client can query the API by user+month.

- **Populate flat/nested on upload.** On receiving a CSV POST, the server should parse it (e.g. using the same logic as the client or a Node CSV library) and insert the parsed rows into the DB. At minimum, store the raw content and separately store the line items (with their hierarchical relationships). If performance isn’t critical, the server could store the entire parsed JSON in one table as JSONB (e.g. columns `parsed_flat JSONB, parsed_nested JSONB`). This simplifies coding at the cost of relational query ability.

- **Choose a database.** The code is already set up for Postgres (using Neon), which is a fine choice for structured data and ACID durability. Alternatively, a NoSQL (e.g. MongoDB) could store each upload as a document with embedded JSON arrays for flat/nested. If easier deployment is desired, Firebase Firestore could also be used (each CSV upload as a document, storing nested objects). But since Drizzle/Postgres is present, continuing with Postgres is recommended for relational queries (e.g. aggregate analytics).


By persisting uploads and parsed data in a database (keyed by user/month/type), data will survive server restarts and can be queried later.

## 3\. API Layer

The frontend currently uses REST endpoints for file upload, but **no endpoints exist for retrieving data or analytics**. We should expand the API to cover all key operations:

- **CSV Upload endpoints (existing).** The backend defines POST `/api/upload/annual` and `/api/upload/monthly/:type` (with `?month=…`)[github.com](https://github.com/0xGonz/Arizona_Practice/blob/8904ebb566592c0a7375f7d9b30b1ae11b0ef148/client/src/components/upload/csv-upload.tsx#L48-L55). These correctly receive file uploads and call `storage.storeCSVFile()`. We should enhance these to actually process the file (parse and save structured data) as noted above.

- **Data retrieval endpoints (new).** The frontend needs to load data for display. For example:


  - **GET `/api/uploads`** – return a list of all uploads (could be per-user). This is already implemented (calls `storage.getCSVUploads()`), but it currently returns no content for each upload (just metadata)[github.com](https://github.com/0xGonz/Arizona_Practice/blob/8904ebb566592c0a7375f7d9b30b1ae11b0ef148/client/src/store/data-store.ts#L519-L526). We should ensure it returns at least the parsed data or an ID that the client can use to fetch the content.

  - **GET `/api/uploads/:id`** – return a single upload record including its CSV content (and possibly parsed data). This exists. We should use it: for example, when the client loads the dashboard, it could call this to re-populate state from the database.

  - **GET `/api/data/annual`** – return the parsed annual CSV (flat rows) plus any computed dashboard values. (Or the client could parse it itself, but an API is cleaner.)

  - **GET `/api/data/monthly?month={m}&type={e|o}`** – return the parsed monthly CSV for the given month and type.

  - **GET `/api/analytics/doctors?month={m}`** – return pre-computed doctor performance data (revenue/expenses per doctor) for a given month or aggregate. Alternatively, compute on-the-fly from saved CSV data.

  - **GET `/api/analytics/departments?month={m}`** – similarly for department metrics.


By adding these endpoints, the frontend can simply fetch JSON and render it, rather than recomputing everything in state on every load. For example, instead of calling local utility functions like `extractDoctorPerformanceData(monthlyData)` in the client, the client could fetch `/api/analytics/doctors?month=january` which returns an array of `{ doctorName, revenue, expenses, net }`.

- **REST vs GraphQL.** The app currently uses REST, and this is fine. GraphQL could be introduced for flexibility (one endpoint for multiple queries), but it may be overkill unless the UI needs very dynamic queries. For now, augmenting the REST API is simpler. If more complex querying is needed (e.g. filtering or aggregations), GraphQL could be considered later.


In summary, define API routes for all key actions (upload, list/uploads by filter, get by ID, and analytics). Make sure the frontend calls these instead of relying solely on local state. For example, on app startup or login, the client should fetch the current `uploadStatus`, `annualData`, and `monthlyData` from the server (not just localStorage).

## 4\. Frontend-Backend Integration

Right now, the frontend _both_ uploads to the server **and** immediately parses the file in the browser, storing results in Zustand and localStorage[github.com](https://github.com/0xGonz/Arizona_Practice/blob/8904ebb566592c0a7375f7d9b30b1ae11b0ef148/client/src/components/upload/csv-upload.tsx#L48-L55)[github.com](https://github.com/0xGonz/Arizona_Practice/blob/8904ebb566592c0a7375f7d9b30b1ae11b0ef148/client/src/components/upload/csv-upload.tsx#L72-L81). This means the “backend” (server) is only used as a file sink, and all derived data remains only on the client. This causes the reported issue: if the page or browser closes, the React state is lost (and relying on localStorage is fragile, especially across devices or browsers).

To fix this, the integration must be two-way:

- **Backend as the source of truth.** After a successful upload POST, the server should parse and save the data (as above). The client upload component can still parse immediately for responsiveness, but it should then discard its own parsed data in favor of fetching the authoritative copy from the server. For example, once upload completes, the client can call `/api/data/monthly?month=january&type=e` and populate the store from that JSON. That way, reloading the app will fetch the same data. In code, this means adding `useEffect` hooks or similar to fetch the data when components mount (instead of reading only from Zustand).

- **Eliminate in-memory-only storage.** CSV files should not be kept only in memory. The client currently does `const processedData = parseMonthlyCSV(data,type)` and merges it into state[github.com](https://github.com/0xGonz/Arizona_Practice/blob/8904ebb566592c0a7375f7d9b30b1ae11b0ef148/client/src/store/data-store.ts#L553-L561)[github.com](https://github.com/0xGonz/Arizona_Practice/blob/8904ebb566592c0a7375f7d9b30b1ae11b0ef148/client/src/store/data-store.ts#L579-L587). Instead, move `parseMonthlyCSV` to the server. The client should send the raw file to `/api/upload`, and the server handler should call `parseMonthlyCSV` (or its own parser) on `req.file.buffer`. Then the handler returns the processed JSON to the client (or simply a success code and the client fetches it later). This ensures both sides use the same logic and the client isn’t the sole holder of parsed data.

- **Example refactor:** Modify `/api/upload/monthly/:type` so that after `const fileContent = req.file.buffer.toString()`, the server does `const parsed = parseMonthlyCSV(...)` and saves `parsed` (flat and nested) in the DB. It can then respond with something like `{ uploadId, parsed }`. On the client side, instead of storing `data` in localStorage (as a fallback), call the store’s update via an API fetch. Update the Zustand store by dispatching the JSON returned (or from a subsequent GET). Essentially, the store mutation would happen _after_ fetching from backend, not from local parse.

- **LocalStorage as cache only.** The client code already writes to `localStorage` in many places (see `localStorage.setItem('annualData', ...)` in \[28†L504-L513\], and similarly for monthly in \[29†L602-L611\]). This localStorage syncing can remain as a cache layer, but **data should be loaded from the backend on startup**. For example, on app load check localStorage for a “user token” or ID; then fetch `/api/uploads` and `/api/data/*` to populate the store. The localStorage writes then help with quick reloads but are not the master copy.


With this refactor, even if the browser closes (or a user switches to another device), the data lives in the backend DB, and the app can recover it from the API. In short, shift the authoritative storage to the server and use API fetches to keep the frontend state in sync.

## 5\. Session and User State

Currently the app has no user authentication or per-user state. The store has a generic `uploadHistory` and flags in `uploadStatus`, but these are global to the app on that device. To support multi-session persistence and multi-user use, we recommend:

- **Introduce user accounts or identifiers.** At minimum, generate a user ID (e.g. via login or anonymously) and include it in all API calls. For full user management, add authentication (e.g. JWT, OAuth, or Firebase Auth) so that each user’s uploads and analytics are scoped to them. Then the backend can filter queries by `user_id`. The `csv_uploads` table should include a `user_id` foreign key (or at least a unique user token) so you can query “uploads for this user”.

- **Session continuity via tokens or cookies.** If implementing auth, use secure HTTP-only cookies or localStorage tokens to identify the user on each request. Ensure token renewal or session expiry is handled. If not implementing full auth, at least store a unique session ID in localStorage and pass it with each API call so the server can tie uploads to that pseudo-user.

- **Persist upload/analysis across closures.** Right now, data survives only if localStorage isn’t cleared. With a backend store, we can load it on any session. The client UI should allow a user to see their previous uploads and analyses. For example, an “Upload History” page could fetch `/api/uploads` (for that user) and display previous files. Deleting an upload would call the DELETE API or `clearUploadedData(type,month)` which currently only clears localStorage; it should also call the server to delete those records. (E.g. add an endpoint DELETE `/api/uploads/:id` that removes the CSV and its parsed rows from DB.)

- **Sync upload status.** The existing `uploadStatus` flags in state (annual/monthly done) are only local flags. Instead, store upload timestamps or status in the DB. When fetching data, also retrieve whether each expected upload is present. Then the UI can show “uploaded” or “not yet” for each month/type based on actual data in DB, not a local flag.


By introducing a user/session identity and moving persistence to the server, user data will no longer be lost when they close the browser. Uploads and analysis become tied to the user in the database, and loading the app will rehydrate the state from the server.

## 6\. Cloud and Hosting Readiness

The app’s stack (Vite + Express + Neon/Postgres) can be deployed in several ways, but some adjustments will make it production-ready:

- **Environment Configuration.** Ensure all environment-specific settings (e.g. `DATABASE_URL`, API base URL, CORS origins) are configurable via environment variables. The code already checks `process.env.DATABASE_URL` to switch storage modes. In deployment, set `DATABASE_URL` to the production DB. Also set NODE\_ENV to “production” and ensure secrets (DB creds, JWT secrets if added) are not checked into code.

- **Build and Static Serving.** The repo includes `vite.config.ts` and a `server/vite.ts` that likely handles development SSR. For production, you might build the client ( `npm run build`) and serve the static files. For example, you could configure Express to serve the built `dist` folder. Vercel or Netlify expect a static build; for AWS, you could run Express on an EC2 or ECS container that serves both the API and built client. The `server/vite.ts` and `.replit` files suggest this was designed to run on Replit, but for cloud hosting you may separate concerns: host the React app as static, and host the Express API separately.

- **Platform choices.** \- _Vercel:_ You can deploy the frontend (created by Vite) easily to Vercel as a static site, and use Vercel Serverless Functions for the API routes. However, the Express server would need to be refactored into API routes compatible with Vercel’s serverless functions (or use the @vercel/node builder). Alternatively, use Vercel for frontend and deploy the backend to a separate service (e.g. Heroku, AWS).
  - _Firebase:_ You could use Firebase Hosting for the static React app and Firebase Cloud Functions for the API (rewriting the Express routes as functions). For data, instead of Postgres you could use Firestore or Firebase Realtime Database. That would require a rewrite of storage (but the logic could be similar). Firebase Authentication could easily handle user sessions.

  - _AWS:_ On AWS, you could deploy Express in a Docker container (ECS/Fargate) or on Lambda via Serverless framework. The React app could go to S3 + CloudFront. Postgres can be run in RDS or Aurora (Neon is compatible with PostgreSQL). Add an Application Load Balancer and autoscaling if needed. Use AWS IAM roles for secure DB access and parameter store for secrets.
- **Security and scalability.** Whichever platform, run the app over HTTPS. Use CORS middleware to restrict API access to your domains. Sanitize and validate all CSV inputs on the server to avoid injection. The current multer limit is 10MB, which should be safe, but ensure large files are streamed to avoid memory spikes. If expecting high load, use a stateless API design (e.g. store sessions in the database or JWT rather than in-memory) and consider horizontal scaling for the API servers. For durability, ensure the database is backed up and uses multiple availability zones.

- **Data flow.** With a proper API, the client and server communicate over HTTPS with JSON. We have debug logging in server ( `log(...)` in `vite.ts`) which is fine for dev but in production use a logger that writes to a file or logging service. Remove any dummy data (the dev message “saved to the database” in toasts \[32†L245-L253\] should be updated to reflect real DB saves).


In summary, the app can be deployed to any modern host once its storage concerns are addressed. For example, using AWS: React static files on S3/CloudFront, Express on ECS/Fargate with Neon/Postgres for DB. Or on Vercel: split front/back or use serverless functions. The key is ensuring environment variables for DB are set, adding an authentication layer, and moving CSV data into persistent storage (Postgres or a cloud DB) to support durability and scaling.

Each recommendation above promotes clean separation of UI (React) vs logic (parsing, state) vs data (database). By persisting data and using APIs for all data flow, the app will avoid the current “data disappears on reload” issue and be ready for a production deployment.

**Sources:** We inspected the live codebase (e.g. the Zustand store in _data-store.ts_ [github.com](https://github.com/0xGonz/Arizona_Practice/blob/8904ebb566592c0a7375f7d9b30b1ae11b0ef148/client/src/store/data-store.ts#L315-L322)[github.com](https://github.com/0xGonz/Arizona_Practice/blob/8904ebb566592c0a7375f7d9b30b1ae11b0ef148/client/src/store/data-store.ts#L579-L587) and the upload component _csv-upload.tsx_ [github.com](https://github.com/0xGonz/Arizona_Practice/blob/8904ebb566592c0a7375f7d9b30b1ae11b0ef148/client/src/components/upload/csv-upload.tsx#L48-L55)[github.com](https://github.com/0xGonz/Arizona_Practice/blob/8904ebb566592c0a7375f7d9b30b1ae11b0ef148/client/src/components/upload/csv-upload.tsx#L72-L81)) to see how state and API calls are implemented. These show that monthly uploads are only kept in client state and localStorage, and that no nested hierarchy is ever saved[github.com](https://github.com/0xGonz/Arizona_Practice/blob/8904ebb566592c0a7375f7d9b30b1ae11b0ef148/attached_assets/content-1747443284525.md#L20-L28). Our recommendations are based on those findings.

Citations

[![Favicon](https://www.google.com/s2/favicons?domain=https://github.com&sz=32)\\
\\
net-margin-chart.tsx\\
\\
https://github.com/0xGonz/Arizona\_Practice/blob/468135e3ab3e7021484294797b8d41c4179e9e8b/client/src/components/dashboard/net-margin-chart.tsx#L12-L21](https://github.com/0xGonz/Arizona_Practice/blob/468135e3ab3e7021484294797b8d41c4179e9e8b/client/src/components/dashboard/net-margin-chart.tsx#L12-L21) [![Favicon](https://www.google.com/s2/favicons?domain=https://github.com&sz=32)\\
\\
data-store.ts\\
\\
https://github.com/0xGonz/Arizona\_Practice/blob/8904ebb566592c0a7375f7d9b30b1ae11b0ef148/client/src/store/data-store.ts#L315-L322](https://github.com/0xGonz/Arizona_Practice/blob/8904ebb566592c0a7375f7d9b30b1ae11b0ef148/client/src/store/data-store.ts#L315-L322) [![Favicon](https://www.google.com/s2/favicons?domain=https://github.com&sz=32)\\
\\
csv-upload.tsx\\
\\
https://github.com/0xGonz/Arizona\_Practice/blob/8904ebb566592c0a7375f7d9b30b1ae11b0ef148/client/src/components/upload/csv-upload.tsx#L48-L55](https://github.com/0xGonz/Arizona_Practice/blob/8904ebb566592c0a7375f7d9b30b1ae11b0ef148/client/src/components/upload/csv-upload.tsx#L48-L55) [![Favicon](https://www.google.com/s2/favicons?domain=https://github.com&sz=32)\\
\\
csv-upload.tsx\\
\\
https://github.com/0xGonz/Arizona\_Practice/blob/8904ebb566592c0a7375f7d9b30b1ae11b0ef148/client/src/components/upload/csv-upload.tsx#L72-L81](https://github.com/0xGonz/Arizona_Practice/blob/8904ebb566592c0a7375f7d9b30b1ae11b0ef148/client/src/components/upload/csv-upload.tsx#L72-L81) [![Favicon](https://www.google.com/s2/favicons?domain=https://github.com&sz=32)\\
\\
data-store.ts\\
\\
https://github.com/0xGonz/Arizona\_Practice/blob/8904ebb566592c0a7375f7d9b30b1ae11b0ef148/client/src/store/data-store.ts#L579-L587](https://github.com/0xGonz/Arizona_Practice/blob/8904ebb566592c0a7375f7d9b30b1ae11b0ef148/client/src/store/data-store.ts#L579-L587) [![Favicon](https://www.google.com/s2/favicons?domain=https://github.com&sz=32)\\
\\
content-1747443284525.md\\
\\
https://github.com/0xGonz/Arizona\_Practice/blob/8904ebb566592c0a7375f7d9b30b1ae11b0ef148/attached\_assets/content-1747443284525.md#L20-L28](https://github.com/0xGonz/Arizona_Practice/blob/8904ebb566592c0a7375f7d9b30b1ae11b0ef148/attached_assets/content-1747443284525.md#L20-L28) [![Favicon](https://www.google.com/s2/favicons?domain=https://github.com&sz=32)\\
\\
department-utils-new.ts\\
\\
https://github.com/0xGonz/Arizona\_Practice/blob/8904ebb566592c0a7375f7d9b30b1ae11b0ef148/client/src/lib/department-utils-new.ts#L6-L14](https://github.com/0xGonz/Arizona_Practice/blob/8904ebb566592c0a7375f7d9b30b1ae11b0ef148/client/src/lib/department-utils-new.ts#L6-L14) [![Favicon](https://www.google.com/s2/favicons?domain=https://github.com&sz=32)\\
\\
department-utils-new.ts\\
\\
https://github.com/0xGonz/Arizona\_Practice/blob/8904ebb566592c0a7375f7d9b30b1ae11b0ef148/client/src/lib/department-utils-new.ts#L72-L80](https://github.com/0xGonz/Arizona_Practice/blob/8904ebb566592c0a7375f7d9b30b1ae11b0ef148/client/src/lib/department-utils-new.ts#L72-L80) [![Favicon](https://www.google.com/s2/favicons?domain=https://github.com&sz=32)\\
\\
data-store.ts\\
\\
https://github.com/0xGonz/Arizona\_Practice/blob/8904ebb566592c0a7375f7d9b30b1ae11b0ef148/client/src/store/data-store.ts#L519-L526](https://github.com/0xGonz/Arizona_Practice/blob/8904ebb566592c0a7375f7d9b30b1ae11b0ef148/client/src/store/data-store.ts#L519-L526) [![Favicon](https://www.google.com/s2/favicons?domain=https://github.com&sz=32)\\
\\
data-store.ts\\
\\
https://github.com/0xGonz/Arizona\_Practice/blob/8904ebb566592c0a7375f7d9b30b1ae11b0ef148/client/src/store/data-store.ts#L553-L561](https://github.com/0xGonz/Arizona_Practice/blob/8904ebb566592c0a7375f7d9b30b1ae11b0ef148/client/src/store/data-store.ts#L553-L561)

All Sources

[![Favicon](https://www.google.com/s2/favicons?domain=https://github.com&sz=32)github](https://github.com/0xGonz/Arizona_Practice/blob/468135e3ab3e7021484294797b8d41c4179e9e8b/client/src/components/dashboard/net-margin-chart.tsx#L12-L21)