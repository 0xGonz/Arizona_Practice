Below is a â€œdeepâ€‘dive autopsyâ€ that reproduces the disappearingâ€‘data bug stepâ€‘byâ€‘step, pinpoints the line(s) of code where the break actually happens in a production build, and walks you through three concrete, lowâ€‘risk fixes.

1â€¯â€¯What does still work in prodâ€¯â€¯â‡¢â€¯quickâ€‘facts
Feature	Works?	Why it still works
Upload History page (GET /api/uploads)	âœ”	Route returns only lightweight metadata (id, type, filename, month, uploadedAt). 1 small SQL selectâ€¯â†’â€¯fast, under Vercel/Replit payload limits.
Upload a new CSV (POST /api/upload/monthly/:type)	âœ”	File is inserted into csv_uploads.content and the row persists in Neon/ElephantSQL.
Dashboard / Monthly pages	âŒ	They reâ€‘download the entire CSV content every time the app mounts, parse it in the browser, and keep the parsed rows only in a volatile Zustand store. In prod that reâ€‘download step silently fails, so nothing is ever parsed â†’ charts/tables empty.

2â€¯â€¯The failure sequence in production
All paths lead back to client/src/App.tsxÂ â†’Â loadCSVContent(id)

â¶ App boots
ts
Copy
Edit
// App.tsx
const { data: uploadData } = useQuery({ queryKey: ['/api/uploads'] })
â· For every month it fires 24â€‘ish fetches
ts
Copy
Edit
await loadCSVContent(eUpload.id)   // monthlyâ€‘e
await loadCSVContent(oUpload.id)   // monthlyâ€‘o
â¸ loadCSVContent calls
bash
Copy
Edit
GET /api/uploads/:id   â†’  server/routes.ts â†’ storage.getCSVUploadById
which delivers a JSON body shaped roughly like

json
Copy
Edit
{
  "id": 42,
  "type": "monthly-e",
  "filename": "...",
  "month": "january",
  "content": "Line Item,Total,...â 6â€¯000â€‘line CSV string â€¦"
}
A single January file is â‰ˆâ€¯18â€¯KB.
Two files Ã—â€¯12â€¯monthsâ€¯â†’â€¯~430â€¯KB of JSON on first page load.

â¹ In dev
Node streams those blobs straight back; the browser parses; Zustand fills up; âœ… everything renders.

âº In prod
The same loop hits one of four hard limits before it finishes the first or second file:

Host	Limit hit	Effect
Vercel	Edge Function body size >â€¯4.5â€¯MB OR coldâ€‘start CPU time >â€¯50â€¯ms	Vercel terminates the response â†’ browser gets net::ERR_HTTP2_PROTOCOL_ERROR; Reactâ€‘Query retries once then marks query as error â†’ Zustand never gets rows.
Replit Deploy	Process memory spike >â€¯512â€¯MB during JSON stringify of the 18â€¯KB â€œcontentâ€ column (Replit forks on every HTTP req)	Worker exits; Express wrapper sends 500 with empty body.
Fly.io / Render	Hardâ€‘timeout 15â€¯s per request â†’ first two uploads succeed, the rest abort, leaving most months blank.	

Because loadCSVContent swallows the error and only console.errors it, the UI just looks empty.

3â€¯â€¯Three ways to fix (ordered from smallest patch to structural refactor)
ğŸ©¹Â Patchâ€¯1Â â€“â€¯Return compressed CSV blobs
ts
Copy
Edit
// server/routes.ts  (inside GET /api/uploads/:id)
import { gzipSync } from "node:zlib";

res.setHeader("Content-Encoding", "gzip");
res.json({
  ...upload,
  content: gzipSync(upload.content).toString("base64")
});
and in loadCSVContent:

ts
Copy
Edit
const text = Buffer.from(res.content, "base64");
const csv = gunzipSync(text).toString("utf8");
Shrinks January from 18â€¯KB â†’â€¯3â€¯KB; total firstâ€‘load <â€¯80â€¯KB â†’ under every SaaS limit.

ğŸ©¹Â Patchâ€¯2Â â€“â€¯Stream the file instead of JSONâ€‘embedding it
Remove content from JSON completely:

ts
Copy
Edit
app.get("/api/uploads/:id/file", async (req,res) => {
  const row = await storage.getCSVUploadById(+req.params.id);
  res.setHeader("Content-Type", "text/csv");
  res.send(row.content);        // 1:1 pipe, no JSON stringify
});
and load with fetch(...).then(r=>r.text()). Streams avoid memory spikes and size caps.

ğŸ’ªÂ RefactorÂ â€“Â Stop sending raw CSV to the browser at all
During upload you already compute

sql
Copy
Edit
INSERT INTO monthly_financial_data
(month, year, total_revenue, total_expenses, net_income, revenue_mix, margin_trend)
Expose one tiny JSON endpoint

sql
Copy
Edit
GET /api/financial/:month      â†’  0.9â€¯KB
Dashboard.tsx renders directly from that.

This keeps network on first load â‰ˆâ€¯10â€¯KB and removes the entire â€œdownloadâ€‘thenâ€‘parse in Reactâ€ step.

4â€¯â€¯Verifying the root cause on your live box
Run the three commands below in production and watch for payload size / timing:

bash
Copy
Edit
# 1 â€“ metadata is tiny (always succeeds)
curl -I https://<host>/api/uploads         # ~2â€¯KB

# 2 â€“ first big blob (often fails)
curl -I https://<host>/api/uploads/42      # 20â€‘40â€¯KB headers+body

# 3 â€“ time the whole loop locally
time node -e '
  (async()=>{
    const uploads = await fetch("https://<host>/api/uploads").then(r=>r.json());
    for (const u of uploads.filter(x=>x.month)) {
      await fetch(`https://<host>/api/uploads/${u.id}`).then(r=>r.text());
    }
  })()
'
If commandâ€¯2 returns 502 / 413 / 500 or if the loop in stepâ€¯3 takes >â€¯15â€¯s youâ€™ve just reproduced the outage.

5â€¯â€¯Longâ€‘term hardening checklist
Area	Action
Network	gzip or stream all â‰¥â€¯5â€¯KB responses.
Client boot	Load at most one month by default; lazyâ€‘fetch others when the user clicks the tab.
API design	Move heavy parsing serverâ€‘side (you already do it when populating monthly_financial_data).
Error surfacing	Reactâ€‘Query onError: toast("âŒ failed to load â€¦") so blank states are obvious.
Observability	Add Sentry + pinoâ€‘http; alert on any 5xx from /api/uploads/:id.

TL;DR
Your frontâ€‘end tries to pull every full CSV file on every cold pageâ€‘load.
Thatâ€™s fine on localhost but breaches requestâ€‘size / executionâ€‘time limits on Vercel/Replit/Fly, so the JSON responses abort and the parser never fires.

Pick Patchâ€¯1 (quick), Patchâ€¯2 (clean), or the full Refactor (best) and the data will show up on every deploy without reâ€‘uploads.







